{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <span style=\"color:red\">Taxi-v3 With Q-Learning</span>\n",
    "\n",
    "### The Taxi Problem from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\" by Tom Dietterich\n",
    "\n",
    "## [Taxi-v3](https://gym.openai.com/envs/Taxi-v3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random \n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Observations: \n",
    "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger <br>\n",
    "    (including the case when the passenger is in the taxi), and 4 destination locations. 25*5*4 = 500\n",
    "    \n",
    " ##   Passenger locations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "    \n",
    " ##   Destinations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "        \n",
    "  ##  Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east \n",
    "    - 3: move west \n",
    "    - 4: pickup passenger\n",
    "    - 5: dropoff passenger\n",
    "    \n",
    "##    Rewards: \n",
    "    There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger.<br>\n",
    "     There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of observation space:  500\nNumber of action space:  6\n+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| : : : : |\n| | : | : |\n|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n+---------+\n\n"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "print(\"Number of observation space: \", env.observation_space.n)\n",
    "print(\"Number of action space: \", env.action_space.n)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample rendering of Taxi-v3 witout training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---------+\n|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| :\u001b[43m \u001b[0m: : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (East)\nTotal reward without training:  -149\n"
    }
   ],
   "source": [
    "env.reset()\n",
    "total_reward = 0\n",
    "for i in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(action);\n",
    "    total_reward+=reward\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(.2)\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Total reward without training: \", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learing a Temporal-difference(TD) algorithm\n",
    "\n",
    "## Step of Q-learing\n",
    "\n",
    "- 1: First, we initialize the Q function to some arbitrary values.\n",
    "- 2: We take an action from a state using epsilon-greedy policy ( ) and move it. to the new state \n",
    "- 3: We update the Q value of a previous state by following the update rule.\n",
    "- 4: We repeat the steps 2 and 3 till we reach the terminal state\n",
    "\n",
    "## Q-Table euqtion:\n",
    "\n",
    " $Q(s,a)  = Q(s,a)+ \\alpha(r + \\gamma  max(Q( s{}', a ))  - Q(s,a))$\n",
    "\n",
    "\n",
    "### Here\n",
    "- a = action\n",
    "- s = observation space\n",
    "- r = reward\n",
    "- Q - qtable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initializing alpha, gamma and epsilon\n",
    "\n",
    "Initializing Q-talbe : q = size [observation_space, action_space]\n",
    "\n",
    "'''\n",
    "alpha =0.4\n",
    "gamma = 0.999\n",
    "epsilon = 0.017\n",
    "\n",
    "q =np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First calculated the max( Q(s,a) ) here iterate all action of the observation sapce s.\n",
    "\n",
    "'''\n",
    "def update_q_table(prev_state, action, reward, nextstate, alpha, gamma):\n",
    "    qa = max([q[nextstate,a] for a in range(env.action_space.n)])\n",
    "\n",
    "    q[prev_state, action] += alpha*(reward+gamma*qa-q[prev_state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We generate some random number in uniform distribution and if the number is less than the epsilon,\n",
    "we explore a different action in the state,\n",
    "or else we exploit the action that has a maximum q value\n",
    "'''\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if (random.uniform(0,1)< epsilon):\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax([q[state,a] for a in range(env.action_space.n)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training our Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Step : 0 || Total Reward: -587\nStep : 1000 || Total Reward: 10\nStep : 2000 || Total Reward: 9\nStep : 3000 || Total Reward: 5\nStep : 4000 || Total Reward: 7\nStep : 5000 || Total Reward: 7\nStep : 6000 || Total Reward: 10\nStep : 7000 || Total Reward: -2\nStep : 8000 || Total Reward: 9\nStep : 9000 || Total Reward: 7\nStep : 10000 || Total Reward: 7\nStep : 11000 || Total Reward: 8\nStep : 12000 || Total Reward: 10\nStep : 13000 || Total Reward: 8\nStep : 14000 || Total Reward: 3\nStep : 15000 || Total Reward: 10\nStep : 16000 || Total Reward: 3\nStep : 17000 || Total Reward: 11\nStep : 18000 || Total Reward: 7\nStep : 19000 || Total Reward: 5\n"
    }
   ],
   "source": [
    "'''\n",
    "First  chose a action by our epsilon_greedy_policy\n",
    "Then take a setp by this action and calculate next state and reward\n",
    "After that we update our q-table with next stete and reward\n",
    "'''\n",
    "\n",
    "env.reset()\n",
    "for i in range(20000):\n",
    "    total_reward =0\n",
    "\n",
    "    prev_state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = epsilon_greedy_policy(prev_state, epsilon)\n",
    "        nextstate, reward, done, _ = env.step(action)\n",
    "        update_q_table(prev_state, action, reward, nextstate,alpha, gamma)\n",
    "\n",
    "        prev_state = nextstate\n",
    "\n",
    "        total_reward +=reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    if i%1000 ==0:\n",
    "        print(\"Step : {} || Total Reward: {}\".format(i,total_reward))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now play Taxi-v3 with our q table\n",
    "\n",
    "- Here we play several episode. \n",
    "- Each episode is individual game.\n",
    "- First Taxi goes in the station where people are that station\n",
    "- Then took that people in taxi and drop in destination\n",
    "\n",
    "## Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - yellow: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters (R, G, Y and B): locations for passengers and destinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---------+\n|R: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n+---------+\n  (Dropoff)\n"
    },
    {
     "data": {
      "text/plain": "None"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Episode:  4\nNumber of steps taken:  12\nReward:  8\n"
    }
   ],
   "source": [
    "env.reset()\n",
    "max_steps = 100\n",
    "# lets test for 5 episodes\n",
    "for episode in range(5):\n",
    "    state = env.reset();\n",
    "    step = 0;\n",
    "    done = False;\n",
    "    total_reward =0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #take the action with maximum expected future reward form the q-table\n",
    "        action = np.argmax(q[state,:]);\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action);\n",
    "        display.display(env.render())\n",
    "        time.sleep(.25)\n",
    "        # display.clear_output(wait=True)\n",
    "        total_reward+=reward\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode: \",episode);\n",
    "            print(\"Number of steps taken: \",step);\n",
    "            print(\"Reward: \", total_reward)\n",
    "            #env.render();\n",
    "            time.sleep(3)\n",
    "            \n",
    "            break;\n",
    "        display.clear_output(wait=True)    \n",
    "        state = new_state;\n",
    "\n",
    "#close the connection to the environment\n",
    "env.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}